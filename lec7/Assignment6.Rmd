---
title: "Homework Six"
author: "Akshay	Tandon,	Nathan	Matare,	Linda	Xie	"
date: "3/1/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Data Prep, include = FALSE, echo = FALSE, results = 'hide'}

	options("width" = 200)
	library(bit64)
	library(data.table)
	library(glmnet)
	library(ranger)
	library(ggplot2)
	library(corrplot)
	library(knitr)
	library(data.table)
	library(glmnet)
	library(parallel)
	library(boot)

	username <- Sys.info()[["user"]]
	dir <- paste("/home/", username, "/projects/datascience_marketing/lec7/", sep = ""); 
	setwd(dir)
		load("Customer-Development.RData")

```

## Data Description 

Our dataset is heavily unbalanced; most customers did not purchase. However, of those who did purchase, we observe 21,375 unique purchases. Conditional on purchasing, the average amount spent is 111 dollars in a total range of 14 to 1100 dollars.

```{r data descrp, include = TRUE, tidy.opts = list(width.cutoff = 20), results = "hide", echo = FALSE}	

set.seed(1999)
crm_DT[, training_sample := rbinom(nrow(crm_DT), 1, 0.5)]

spend_DT = crm_DT[outcome_spend >= 0, .(customer_id, mailing_indicator, outcome_spend)]
spend_DT[outcome_spend == 0, purchase_incidence := 0]
spend_DT[outcome_spend > 0, purchase_incidence := 1]
spend_DT_purchases = spend_DT[outcome_spend > 0]

ggplot(spend_DT, aes(x = outcome_spend)) + geom_histogram() + theme_bw()
sum(spend_DT$purchase_incidence)

ggplot(spend_DT_purchases, aes(x = outcome_spend)) + geom_histogram() + theme_bw() + xlim(0, 300)
summary(spend_DT_purchases$outcome_spend)

```	



\newpage
## Correlation Plot

We calculate a correlation matrix. 

```{r prepare move, include = TRUE, tidy.opts = list(width.cutoff = 20), hidden = TRUE, cache = TRUE}	

cor_matrix <- cor(crm_DT[, !c("customer_id", "mailing_indicator", "outcome_spend"), with = FALSE])
cor_matrix[upper.tri(cor_matrix, diag = TRUE)] <- NA
cor_DT <- data.table(row = rep(rownames(cor_matrix), ncol(cor_matrix)),
					 col = rep(colnames(cor_matrix), each = ncol(cor_matrix)),
					 cor = as.vector(cor_matrix))
cor_DT <- cor_DT[is.na(cor) == FALSE]

remove <- cor_DT[cor > 0.95 | cor < -0.95, row] # remove correlation greater than 95%
crm_DT <- crm_DT[ ,!remove, with = FALSE] # new DT

```

For illustration, we show the correlation among the first three features:

```{r cor_matrix, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}
cor_matrix[1:3, 1:3] # see first three corplots
```

Six features are highly correlated. We remove them.
```{r removed, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE, echo = FALSE}
remove
```

\newpage
## Predictive Models 

Next we fit four models. In order to speed up computation we only search through ten possible values of alpha [0.1 - 1.0]
```{r predictive models, hidden = TRUE, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE,results="hide"}

# Cut data
train <- crm_DT[training_sample == 1][ ,training_sample := NULL][]
test <- crm_DT[training_sample == 0][ ,training_sample := NULL][]

# OLS
ols.model <- glm(outcome_spend ~., data = train)
coef.ols <- coef(ols.model)

# LASSO
y <- as.numeric(train$outcome_spend)
x <- data.matrix(train[ ,-grep("outcome_spend", colnames(train)), with = FALSE])
xx <- data.matrix(test[ ,-grep("outcome_spend", colnames(test)), with = FALSE])
lasso <- cv.glmnet(x = x, y = y, alpha =  1, family = "gaussian", nfolds = 10) # CV.lasso
coef.lasso <- coef(lasso, s = "lambda.min")

# ELASTIC NET
mse_eval <- data.table(alphas = seq(0.1, 1, by = 0.1), mse = 0)
for(i in 1:NROW(mse_eval)){
	elastic.net <- cv.glmnet(x = x, y = y, alpha =  mse_eval$alphas[i], 
	                         family = "gaussian", nfolds = 10) # e/n
	mse_eval[i, mse := min(elastic.net$cvm)]
	print(paste("Finished", i, "of", NROW(mse_eval)))
}

best.alpha <- mse_eval[which.min(mse_eval$mse)]$alphas # 0.1
elastic.net <- cv.glmnet(x = x, y = y, alpha =  0.1, family = "gaussian") # re-run cv at best alpha
coef.elastic.net <- coef(elastic.net, s = "lambda.min")

# RF
random.forest <- ranger(outcome_spend ~., data = train, num.trees = 2000, 
                        write.forest = TRUE, num.threads = detectCores() - 1, 
                        verbose = FALSE)
	
```

Naturally, our naive linear model attempts to fit all features into the regression. We could implement a forward stepwise regression in order to isolate and include the most explanatory variables. However, we would inevitably introduce false positives and likely incorporate much multicollinearity into our estimated regression.

In order to account for these issues, we introduce regularized regressions: the LASSO and elastic net. Here we attempt to select the most important features by placing a penalty weight on each included coefficient. That is, only truly explanatory features are included in the regression. 


Observing our penalty parameter, alpha, we note that the elastic net regularizes our coefficients slightly less than the LASSO. The elastic net is a special case of the LASSO; that is, a combination of both the L1 and L2 norms. Meaning that our coefficients are more heavily penalized. Expectedly, we observe more sparsity in our coefficients given the LASSO model than the elastic net.


```{r coef compare, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}

	coefs <- cbind(coef.ols, coef.lasso, coef.elastic.net)
	colnames(coefs) <- c("OLS", "LASSO", "ELASTIC_NET"); head(coefs, 20)

```

## Model Validation

Next we compare our out-of-sample predictions across four models:

```{r graph comparison, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}
yhat.ols <- predict(ols.model, newdata = test)
yhat.lasso <- predict(lasso, newx = as.matrix(xx), s = "lambda.min")
yhat.elastic.net <- predict(elastic.net, newx = as.matrix(xx), s = "lambda.min")
yhat.forest <- predict(random.forest, data = as.matrix(test))$predictions
```

```{r graphs of preds, echo = FALSE}
ggplot(test, aes(x = outcome_spend, y = yhat.ols)) + geom_point() + ggtitle("True Spend vs OLS")
ggplot(test, aes(x = outcome_spend, y = yhat.lasso)) + geom_point() + ggtitle("True Spend vs LASSO")
ggplot(test, aes(x = outcome_spend, y = yhat.elastic.net)) + geom_point() + ggtitle("True Spend vs Elastic Net")
ggplot(test, aes(x = outcome_spend, y = yhat.forest)) + geom_point() + ggtitle("True Spend vs Forest")

cbind(cor(test$outcome_spend, yhat.ols), 
      cor(test$outcome_spend, yhat.lasso), 
      cor(test$outcome_spend, yhat.elastic.net), 
      cor(test$outcome_spend, yhat.forest)
)

```

Judging from the graphs, we would expect the bagged trees to perform optimally. That is, the relationship between the true value, outcome_spend, and the predicted value from our random forest model appears more strongly linear than the relationship among the other models. Apparently, the feature space is highly interacted. However, upon inspecting the correlation between actual spend and predicted spend, it appears that the elastic net and LASSO are more strongly correlated. Let us use an evaluation metric (MSE) to gain a more robust understanding: 

```{r OOS comparison, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}

getMSE <- function(yhat, true = test$outcome_spend) return(round(mean((yhat - true) ^ 2),4))
cbind(OLS = getMSE(yhat.ols), 
      LASSO = getMSE(yhat.lasso), 
      ELASTIC_NET = getMSE(yhat.elastic.net), 
      RANDOM_FOREST = getMSE(yhat.forest))

```

Our suspicions are confirmed. Indeed, the feature space is more linearly separable than not. Both our elastic net and LASSO do comparably well on the validation set. The optimal model, however, is the LASSO with lowest MSE.

```{r lift function, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE, echo = FALSE}

#Lift Table Function
liftTable <- function(pred_outcome, obs_outcome, response, segments) {
        liftTble = data.table(cbind(pred_outcome, obs_outcome))
        liftTble[, score_group := cut_number(pred_outcome, n = segments)]
        liftTble[, group_rank := as.integer(score_group)]
        avg_sample_resp_rate = mean(response, na.rm = TRUE)
        liftTble[, avg_sample_resp_rate := avg_sample_resp_rate]
        liftTble[, avg_pred_outcome := mean(pred_outcome, na.rm = TRUE), by = score_group]
        liftTble[, avg_obs_outcome := mean(obs_outcome, na.rm = TRUE), by = score_group]
        liftTble[, number_obs := .N, by = score_group]
        liftTble[, lower_bound := avg_obs_outcome - 1.96*(sd(obs_outcome)/sqrt(.N)), by = score_group]
        liftTble[, upper_bound := avg_obs_outcome + 1.96*(sd(obs_outcome)/sqrt(.N)), by = score_group]  
        liftTble[, lift_factor := 100*(avg_pred_outcome/avg_sample_resp_rate)]
        liftTble[, obs_outcome := NULL]
        liftTble = unique(liftTble[ ,.(group_rank, score_group, avg_sample_resp_rate, avg_pred_outcome, avg_obs_outcome, number_obs, lower_bound, upper_bound, lift_factor)])
return(liftTble)
}

```

Below we plot our lift charts with error bars. Expectedly, the lift is smaller for those models that are less accurate.

```{r lift tables, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE, results = "hide", echo = FALSE}
#Create Lift Tables
test[ ,spend :=  ifelse(outcome_spend > 0, 1,0)]
lift_OLS = liftTable(yhat.ols, test$outcome_spend, test$response, 20)
lift_OLS = lift_OLS[order(-group_rank),]

lift_LASSO = liftTable(yhat.lasso, test$outcome_spend, test$response, 20)
lift_LASSO = lift_LASSO[order(-group_rank),]

lift_elastic_net = liftTable(yhat.elastic.net, test$outcome_spend, test$response, 20)
lift_elastic_net = lift_elastic_net[order(-group_rank),]

lift_random_forest = liftTable(yhat.forest, test$outcome_spend, test$response, 20)
lift_random_forest = lift_random_forest[order(-group_rank),]
```

```{r lift charts, cache = TRUE}
#Plot Lifts
ggplot(lift_OLS, aes(x = group_rank, y = avg_obs_outcome)) + geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), color = "deepskyblue2", size = 0.6, width = 0.1) + geom_point(shape = 21, color = "gray30", fill = "hotpink", size = 2.5) + theme_bw() + ggtitle("Lift OLS")

ggplot(lift_LASSO, aes(x = group_rank, y = avg_obs_outcome)) + geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), color = "deepskyblue2", size = 0.6, width = 0.1) + geom_point(shape = 21, color = "gray30", fill = "hotpink", size = 2.5) + theme_bw() + ggtitle("Lift LASSO")

ggplot(lift_elastic_net, aes(x = group_rank, y = avg_obs_outcome)) + geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), color = "deepskyblue2", size = 0.6, width = 0.1) + geom_point(shape = 21, color = "gray30", fill = "hotpink", size = 2.5) + theme_bw() + ggtitle("Lift Elastic Net")

ggplot(lift_random_forest, aes(x = group_rank, y = avg_obs_outcome)) + geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), color = "deepskyblue2", size = 0.6, width = 0.1) + geom_point(shape = 21, color = "gray30", fill = "hotpink", size = 2.5) + theme_bw() + ggtitle("Lift Random Forest")

```

\newpage
## Predictive Modeling

We begin by creating a spending column that indicates whether or not the customer purchased. We remove the amount purchased. The validation data already included an added spend column, but for clarity we rename the data.table.

```{r make data, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}

trainresponse <- copy(train[ ,spend := ifelse(outcome_spend > 0, 1, 0)]) 
# make spend indicator NULL, unknown at time
trainresponse <- trainresponse[ ,outcome_spend := NULL][]
testresponse <- test

```

First we determine the probability of a customer purchasing or not based upon the entire training dataset. Based upon this model, we predict the probability of purchasing in the validation dataset.
```{r try response, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}
logit <- glm(spend ~., data = trainresponse, family = "binomial")
phats <- predict(logit, newdata = testresponse, type = "response")
head(phats)
```

Now that we have obtained the probability of whether a customers will purchase or not, we need to determine an appropriate threshold in order to classify our customers as either spenders or non spenders. We do this by creating a receiver operator curve (ROC) and inspecting the sensitivity and specificity. The probability that maximizes both corresponds to our optimal threshold probability. This is probability value where we have greatest number of true positives and lowest number of false positives.  

```{r ROC, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE, echo = FALSE}
library(pROC)
ROC <- roc(response = testresponse$spend, predictor = unlist(phats))
df <- ROC$sensitivities + ROC$specificities
optimal.phat <- phats[which.max(df)]

```

If the predicted probability is lower than our optimal threshold, we predict that the customer will not spend [0], otherwise the customer will spend [1]. We cut our phats and corresponding validation dataset based upon the optimal threshold. Now, our validation dataset is conditional on our probability predication that only a specific subset of customers will purchase. We can now predict our linear regression model onto these customers. 

```{r cutphats, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}
phats.new <- phats[phats >= optimal.phat]
testresponse <- test[phats >= optimal.phat]
```

We train a regression model on the training dataset conditional on customers purchasing. We predict onto the validation dataset conditional on our expectation that these customers make purchases. 

```{r trainregression, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE}
trainlinear <- copy(train[outcome_spend > 0][ ,spend := NULL])
linear     	<- lm(outcome_spend ~ ., data = trainlinear)
yhats  		<- predict(linear, newdata = testresponse)

linear.log 	<- lm(log(outcome_spend) ~ ., data = trainlinear)
yhats.log 	<- predict(linear.log, newdata = testresponse)
yhats.log   <- exp(yhats.log + ((summary(linear.log)$sigma) ^ 2 / 2))

# Conditional expectation
conditional.expectation <- phats.new * yhats
getMSE(conditional.expectation, true = testresponse$outcome_spend) # MSE comparision
```

Given that our regression estimates are already conditioned on the probability of purchase, we are impressed by our MSE. It is superior to the other models. Our lift plot tells a similiar story. 

```{r lifttables2, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE, echo = FALSE, results = 'hide'}
testresponse[ ,spend :=  ifelse(outcome_spend > 0, 1,0)]
lift_MD = liftTable(yhats, testresponse$outcome_spend, testresponse$response, 20)
lift_MD = lift_MD[order(-group_rank),]
```

```{r lifttables, include = TRUE, tidy.opts = list(width.cutoff = 20), cache = TRUE, echo = FALSE}
ggplot(lift_MD, aes(x = group_rank, y = avg_obs_outcome)) + geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), color = "deepskyblue2", size = 0.6, width = 0.1) + geom_point(shape = 21, color = "gray30", fill = "hotpink", size = 2.5) + theme_bw() + ggtitle("Lift Decomposition")
```