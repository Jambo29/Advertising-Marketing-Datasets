---
title: "Homework Four"
author: "Akshay	Tandon,	Nathan	Matare,	Linda	Xie	"
date: "2/8/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Data Prep, include = FALSE}

	options("width" = 100)
	library(bit64)
	library(data.table)
	library(lfe)
	library(psych)
	library(stargazer)
  library(RcppRoll)
	library(ggplot2)
	library(stringr)
	library(foreach)
	library(doMC)
	library(parallel)

	username <- Sys.info()[["user"]]
	dir <- "/home/johnconnor/Documents/Education/Chicago_Booth/Classes/37105_Data_Science_for_Decision_Making/datascience_marketing/lec4/"
	setwd(dir)
	
	selected_module = 7260
	selected_brand = 526996
	model_name = "Toilet-Tissue-Charmin"

#############
# Data Preparation
#############
	
	load('Brands.RData')
	load('Stores-DMA.RData')

	setwd(paste(dir, "AdIntel", sep = ""))
	load(paste(selected_module, '.RData', sep = ""))

	setwd(paste(dir, "RMS", sep = ""))
	load(paste(selected_module, '.RData', sep = ""))

	setnames(move, "units", "quantity")
	setnames(move, "promo_percentage", "promotion")

```	

## RMS Scanner Data

We prepare the RMS scanner data by calculating average price, promototion and total quantity by brand name, store code, and week-end. 

```{r prepare move, include = TRUE, tidy.opts = list(width.cutoff = 20)}	
	
	move[brand_code_uc == selected_brand, brand_name := 'own']
	move[brand_code_uc != selected_brand, brand_name := 'comp']
	move[ ,brand_code_uc := NULL] # no longer needed
	setkey(move, brand_name, store_code_uc, week_end)

	move[ ,':='(price = mean(price), promotion = mean(promotion), quantity = sum(quantity)), by = .(brand_name, store_code_uc, week_end)] # mean price and total quanity by brand, by store and by week
	move <- unique(move[ ,.(brand_name, store_code_uc, week_end, price, promotion, quantity)]) # remove duplicated comptitors (3 exist because of sum/mean function)

	setkey(stores, store_code_uc)
	stores_dma <- unique(stores[, .(store_code_uc, dma_code)])
	move <- merge(move, stores_dma) # uniquely merge dma_code with move data
```


```{r get codes, include = FALSE, hidden = TRUE}		
	setkey(adv_DT, brand_code_uc, dma_code, week_start)
	brands <- unique(adv_DT$brand_code_uc)
	dma_codes <- unique(adv_DT$dma_code)
	weeks <- seq(from = min(adv_DT$week_start), to = max(adv_DT$week_start), by = "week")
```

## Cross Join

Next we cross-join the adv_DT data.table and impute the NAs

```{r combine join, include = TRUE, hidden = FALSE}		
	adv_DT <- adv_DT[CJ(brands, dma_codes, weeks)]
	adv_DT[is.na(adv_DT)] <- 0

```

```{r select own and comp, include = FALSE, hidden = TRUE}			
	
	# get price and quanity of competitors and quantities
	adv_DT[brand_code_uc == selected_brand, brand_name := 'own']
	adv_DT[brand_code_uc != selected_brand, brand_name := 'comp']
	adv_DT[ ,brand_code_uc := NULL] # no longer needed
	setkey(adv_DT, brand_name, dma_code, week_start)

```

## Create Group Data

Finally, we average the national group data, sum the local group data, and create a 'grp' variable.  

```{r get group level stuff, include = TRUE}			

	adv_DT[ ,':='(	national_grp = sum(national_grp), local_grp = sum(local_grp), 
					grp = sum(national_grp, local_grp)), by = .(brand_name, dma_code, week_start)] # mean price and total quanity by brand, by store and by week
	adv_DT[ ,local_occ := NULL]
	adv_DT <- unique(adv_DT[ ,.(brand_name, dma_code, week_start, grp)]) # remove duplicated comptitors (3 exist because of sum/mean function)

```	


```{r random, include = FALSE, hidden = TRUE}			
	#calc ad stock
	N_lags = 52
	delta = 0.9

	geom_weights <- cumprod(c(1.0, rep(delta, times = N_lags)))
	geom_weights <- sort(geom_weights)
#	tail(geom_weights)

	setkey(adv_DT, brand_name, dma_code, week_start)

	library(RcppRoll)
	adv_DT[, adstock := roll_sum(log(1 + grp), n = N_lags + 1, weights = geom_weights, normalize = FALSE, align = "right", fill = NA), by = .(brand_name, dma_code)]

	#merge data
	adv_DT[, week_end := week_start + 5]

	setkey(move, brand_name, dma_code, week_end)
	setkey(adv_DT, brand_name, dma_code, week_end)

	move <- merge(move, adv_DT)
	move <- dcast(move, dma_code + store_code_uc + week_end ~ brand_name, value.var = c("quantity", "price", "promotion", "adstock"))
	move <- move[complete.cases(move)]

	move[ ,month_index := 12 * (year(week_end) - min(year(week_end))) + month(week_end)]
	move[ ,dma_code := as.factor(dma_code)] # control for store
	move[ ,store_code_uc := as.factor(store_code_uc)] # control for store

```

\newpage
## Plots

We select Chicago IL at code 602.

When inspecting the group level data for Chicago IL, we observe an oscillating pattern. There appears to be a mean-reverting trend centered around 300. Notably, in mid-2012 advertising levels drop significantly before returning to the mean-level; afterwards the oscillation is less apparent. Perhaps a change in strategy altered the advertising levels.


```{r Data Inspection}

	code = 602
	name = 'CHIAGO IL'
	ggplot(data = adv_DT[dma_code == code], aes(y = grp, x = week_end)) + geom_line(color = 'darkgrey') + 
	  stat_summary(fun.y=mean,geom="line", lwd=1, aes(group=1))	# plot

	adv_DT[ ,normalized_grp := 100 * grp / mean(grp), by = .(dma_code)] 	# normalize
	ggplot(data = adv_DT[dma_code == code], aes(normalized_grp)) + geom_histogram()


```

We observe a bi-modal distribution. After normalization, the advertising levels for Chicago appear to be clustered around 80 and 180. There is also a large number of zeros. The bi-modal distribution ostensibly mirrors the change in time-series data from the previous plot. That is, the event in mid-2012 may have altered advertising levels so as to produce two data-generating processes.

\newpage

## Advertising Effect Estimation

```{r Advertising Effect Estimation}

fit_base = felm(log(1+quantity_own) ~ log(price_own) + log(price_comp) + promotion_own + # Base spec
                  promotion_comp | as.factor(store_code_uc) + month_index, data = move)

fit_adstock = felm(log(1+quantity_own) ~ log(price_own) + log(price_comp) + promotion_own + 
                     promotion_comp + adstock_own + adstock_comp | as.factor(store_code_uc) + 
                     month_index, data = move) # Ad Stock

fit_adstock_noFE = felm(log(1+quantity_own) ~ log(price_own) + log(price_comp) + promotion_own + 
                          promotion_comp + adstock_own + adstock_comp | as.factor(store_code_uc), 
                        data = move) # Ad Stock w/o Fixed Effect

#Stargazer
stargazer(fit_base, fit_adstock, fit_adstock_noFE, 
          column.labels = c("Base", "Ad Stock", "Ad Stock no Time FE"), 
          type = "text", dep.var.labels.include = FALSE)

```

After fitting the base model, we observe four highly significant coefficients; indicating that own/comp price and own/comp promotion do, in fact, impact own_demand. 

Intuitively, the inverse sign relationship between log(price_own) and log(price_comp) makes sense. For example consider a 50% increase in price_own versus price_comp a 50% increase in price_own suggests a 68.25 decrease in own_demand (-1.365 * 50) whereas a 50% increase in price_comp suggests a 39.9 increase in own_demand (0.798 * 50). Given basic economics, one would expect that as one lowers own prices, ceteris paribus, demand should increase. 

A similar analysis follows for promotion_own; that is, promoting one owns product naturally increases demand. However, when inspecting the relative size of the coefficients, we observe a small promotion_comp relative to promotion_own. It appears that, although competitor promotion is significant, the effect is rather small (0.021)

After controlling for adstock_own and adstock_comp our coefficients do not change dramatically. Interestingly, however, is the minuscule size of the coefficients on adstock own/comp. We would expect advertising to work; that is, yield large coefficients! What we observe is significant but tiny coefficients. And although the intuition remains, a negative adstock_comp decreases own_demand; the effect is too minuscule to potentially warrant participation (0.002). 

Tellingly, whatever effect advertising does have on own_demand; it seems largely explained by our fixed time effect. That is, when we stop controlling for time-fixed effects, advertising coefficients change in magnitude—suggesting an omitted variable bias. Our price_own and price_comp coefficients change as well.


\newpage
##Border Strategy

```{r Border Strategy}

stores[, border_name := as.factor(border_name)]
setkey(move, store_code_uc, dma_code, week_end)

move = merge(move, stores[on_border == TRUE, .(store_code_uc, border_name)], allow.cartesian = TRUE)
move[, month_index := as.numeric(month_index)] #Convert index to numeric for below analysis

fit_border = felm(log(1+quantity_own) ~ log(price_own) + log(price_comp) + promotion_own +  #Advertising model w Fixed Effect and Interactions
                    promotion_comp | as.factor(store_code_uc) + border_name:month_index, data = move)

fit_border_SE = felm(log(1+quantity_own) ~ log(price_own) + log(price_comp) + promotion_own + #Advertising model w Fixed Effect and Interactions and SE @ DMA level
                       promotion_comp | as.factor(store_code_uc) + border_name:month_index | 0 | 
                       dma_code, data = move)

#Stargazer
stargazer(fit_border, fit_border_SE, 
          column.labels = c("Border", "Border w SE Cluster"), 
          type = "text", dep.var.labels.include = FALSE)

```

Interestingly, when controlling for border effects, both price_own and price_comp increase significantly in magnitude; border stores are highly susceptible to price changes! However, our standard errors increase significantly when compared to the base model. It would appear that border stores, on average, do help explain the variation in own_demand, but the extent to which is non-uniform—indicative of the larger standard-errors. Naturally, promotion_own decreases and similarly displays larger stand-errors. Here, competitor competition is insignificant. Finally, our coefficients stay the same when controlling for standard-error clustering, but the clustering control increases the relative size of our standard errors—making price_comp completely insignificant.


